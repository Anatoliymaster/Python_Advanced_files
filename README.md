## Python_Advanced_files

____

### Ниже представлены решения задач по продвинутому уровню Python.

## 1. Задание Импорт файла csv. 

Возьмите данные по вызовам пожарных служб в Москве за 2015-2019 годы:
https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv. 
Получите из них фрейм данных (таблицу значений). По этому фрейму вычислите среднее значение вызовов пожарных машин в месяц в одном округе Москвы, округлив до целых
Примечание: найдите среднее значение вызовов, без учета года

#### Решение:

```python
import pandas as pd
data = pd.read_csv("https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv", delimiter=";")
print (data["Calls"].mean().round())
```
____

## 2. Задание: данные из нескольких источников

Получите данные по безработице в Москве:
https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv. 
Объедините эти данные индексами (Месяц/Год) с данными из предыдущего задания (вызовы пожарных) для Центральный административный округ:
https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv
Найдите значение поля UnemployedMen в том месяце, когда было меньше всего вызовов в Центральном административном округе.

#### Решение:

```python
import pandas as pd
data1 = pd.read_csv("https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv", delimiter=";")
data1 = data1.set_index(["Year", "Period"])
data2 = pd.read_csv("https://video.ittensive.com/python-advanced/data-5283-2019-10-04.utf.csv", delimiter=";")
data2 = data2.set_index(["AdmArea", "Year", "Month"])
data2 = data2.loc["Центральный административный округ"]
data2.index.names = ["Year", "Period"]
data = pd.merge(data1, data2, left_index=True, right_index=True)
data = data.reset_index()
data = data.set_index("Calls")
data = data.sort_index()
print (data["UnemployedMen"][0:1])
```
____
## 3. Задание: выделение данных

Получите данные по безработице в Москве:
https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv. 
Найдите, с какого года процент людей с ограниченными возможностями (UnemployedDisabled) среди всех безработных (UnemployedTotal) стал меньше 2%.

#### Решение:

```python
import pandas as pd
data = pd.read_csv("https://video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv", delimiter=";")
data["Sum"] = data.apply(lambda x: 100*x[6]/x[7], axis=1)
data = data[data["Sum"] < 2]
data = data.set_index("Year")
data = data.sort_index()
print (data.index[0:1])
```
____

## 4. Задание: предсказание на 2020 год

Возьмите данные по безработице в городе Москва:
video.ittensive.com/python-advanced/data-9753-2019-07-25.utf.csv
> Сгруппируйте данные по годам, и, если в году меньше 6 значений, отбросьте эти годы.
Постройте модель линейной регрессии по годам среднего значения отношения UnemployedDisabled к UnemployedTotal (процента людей с ограниченными возможностями) за месяц и ответьте, какое ожидается значение процента безработных инвалидов в 2020 году при сохранении текущей политики города Москвы?
Ответ округлите до сотых. Например, 2,32

#### Решение:

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
data = pd.read_csv("D:\Обучение уроки\Продвинутый PYTHON\Часть 1 АНАЛИЗ ДАННЫХ\Безработные.csv", delimiter=";")

# после подгрузки нельзя просто разделить "UnemployedDisabled" на UnemployedTotal, т.к. получится отношение средних.  По условии нужно найти средние отношений, средние процента. Поэтому нужно для каждой строчки данных вычислить значение  некторого нового столбца "UDP" - UnemployedDisabledProcent: 100*data["UnemployedDisabled"]/data["UnemployedTotal"]
# Т.о. получим для каждой строки новую серию данных:

data["UDP"] = 100*data["UnemployedDisabled"]/data["UnemployedTotal"]

# Далее отфильтруем по году через метод .groupby и filter c ляюмда ф., где считаем число строк, которые вошли в эту группу  данных, больше 5(т.е. все записи, где их количество меньше 6, мы отбрасываем, и не будем строить статистку): 

data_group = data.groupby("Year").filter(lambda x: x["UDP"].count() > 5)

# т.к. фильтр возвращает не группы, а сами данные, поэтому нужно группы данных заново сгруппировать по году и взять для них  среднее значение: 

data_group = data_group.groupby("Year").mean()

# и после этого можем посмотреть на те данные, на которые хотим применять линейную регрессию:
# приводим индексы группы данных к массиву и изменяем их форму на двумерный массив:
# (np.array - делает одномерный массив из заданного индекса, a reshape делает двумерный массив из нашего списка  размером - размер списка(len) и 1). По факту получается строчка данных. А для "y" используем "UDP" для  каждой группы данных: 

x = np.array(data_group.index).reshape(len(data_group.index), 1)
y = np.array(data_group["UDP"]).reshape(len(data_group.index), 1)
model = LinearRegression()
model.fit(x, y)

# получаем двумерный массив, состоящих из одной ячейки: 

print (np.round(model.predict(np.array(2020).reshape(1, 1)), 2))
[[1.52]]
```
____

## 5. Задание: Получение данных по API

Изучите API Геокодера Яндекса
tech.yandex.ru/maps/geocoder/doc/desc/concepts/input_params-docpage/ и получите ключ API для него в кабинете разработчика.

Выполните запрос к API и узнайте долготу точки на карте (Point) для города Самара.
Внимание: активация ключа Геокодера Яндекса может занимать несколько часов (до суток).

#### Решение:

```python
Самое сложное получить ключ API. После получение нужно изучить формат API, для того чтобы отправить 
запрос. В квадратных скобках необязательные параметры, их достаточно много. И 2 обязательных параметра, которые нужно использовать(в данном запросе): 
- apikey (это ключ API, который можно получить в кабинете разработчика)
- goecode (чуть сложнее: нужно понять, что нужно передать в геокод - либо адрес, либо географические координаты искомого объекта). Т.к. нам требуется по г. Самаре узнать координаты, то нам нужно передать адрес, т.е. просто "Самара" - это и будет наш базовый запрос:

import requests
import json # нужен для разбора ответа, кот. получим в формате json
r = requests.get("https://geocode-maps.yandex.ru/1.x?geocode=Самара&apikey=0494f4b7-56d0-4cb4-a257-66585f726501&format=json&results=1")

# это базовый запрос. Но этого недостаточно. 
# Также есть параметр format - формат ответа(любо xml либо  json, выбираем json - скопируем в запрос)
# Также будет полезно результаты - result - укажем 1 результат, чтобы не путаться в возвращаемых #объектах

# отправляем сформарованный get запрос к API 
# И после этого говорим, что это объект json.loads <string>

geo = json.loads(r.content) # ответ, который мы получаем, разбираем в json
print(geo['response']['GeoObjectCollection']['featureMember'][0]['GeoObject']['Point']['pos'].split(" ")[0])

	50.100202


В ответе есть Point и координаты - 'pos' -50.100202 - искомая широта точки. Нужно ее найти, вычленить из geo. А для этого нужно последовательно сокращать до тех пор, пока не дойдем до самой точки.
Смотрим, что 'featureMember' - это массив(квадратные скобки) и берем первый объект массива - [0] и переходим к geo объекту (GeoObject) - далее переходим к 'Point' и 'pos', разделим по пробелу - .split(" ") и выведем первый объект с индексом [0] - и получаем искомую ширину координаты

```
____

## 6. Задание: получение котировок акций

Получите данные по котировкам акций со страницы:
mfd.ru/marketdata/?id=5&group=16&mode=3&sortHeader=name&sortOrder=1&selectedDate=01.11.2019
и найдите, по какому тикеру был максимальный рост числа сделок (в процентах) за 1 ноября 2019 года.

#### Решение: 

```python
import pandas as pd
import requests
from bs4 import BeautifulSoup
r=requests.get("https://mfd.ru/marketdata/?id=5&group=16&mode=3&sortHeader=name&sortOrder= 1&selectedDate=01.11.2019")
html = BeautifulSoup(r.content)

<table class="mfd-table" id="marketDataList">

# найдем тег <table> по корому будем искать данные. У нее id = "marketdatalist". 
# Можем старгерироваться по id, его id это marketDataList

table = html.find("table", {"id":'marketDataList'})    # дальше перейдем к разбору таблицы
rows = []
trs = table.find_all("tr") 

#  найдем все теги "tr" внутри таблицы и перебирая все теги и говорим, что  в строчку через генератор добавляем получение текста из ячеек  - td.get_text.  А сами td они находятся  как теги tr.find_all('td') на строчку(на тот html код, который был найден на строчку при разборе "tr")  внутри тега "table", т.е. внутри этого тега, мы ищем все теги "tr" и затем в каждом из этих html кодов  мы выбираем все ячейки и делаем из них небольшой список и перезаписываем в переменную "tr":
# tr = [td.get_text(strip=True) for td in tr.find_all('td')]

# Зачем это нужно? 

# Это нужно, потому что м.б. строки нулевой длины(строки без данных, напр. <th) и их нужно просто 
# отбросить  if len(tr) > 0:

for tr in trs:
    tr = [td.get_text(strip=True) for td in tr.find_all('td')]
    if len(tr) > 0:   # отбрасываем нулевые строки. Если длина больше 0, то соот-но добавляем в строку rows
        rows.append(tr)


# добавили выше аргумент strip = True в td.get_text, чтобы выбросить лишние пробелы и переводы строк и т.д.

# Дальше выгрузим в датафрейм весь этот список списков и сразу дадим названия столбцам(сериям данных), 
# посмотрев их сайте :

data = pd.DataFrame(rows, columns=["Тикер", "Дата", "Сделки", "C/рост", "С/%", "Закрытие", "Открытие", "min", "max", "avg", "шт", "руб", "Всего"])

# нам была нужна колонка Тикер, но для полноты данных все серии отобразим

# Выбросим из DataFrame отсутствующие сделки(N/A), чтобы они не влияли на сортировку процента по сделкам, 
# иначе они могут сбить ключи. Для этого сделаем фильтрацию:

data = data[data["Сделки"] != "N/A"]

# говорим, что все данные берем из всех данных, в том случае, если значение серии "Сделки" не равно N/A.

# Также преобразуем столбец процентов(в столбце есть + и -), и для сортировки по возрастанию и убыванию
# нужно привести его в числу: уберем из столбца дополнительные символы и если посмотреть внимательней
# " - " не является минусом, а дефисом(-). (для красоты используют его, и это для работы с данными неприменима
# И также удаляем знак процента(%) из данных, и приводим к типу float через метод pandas .astype:

data["С/%"] = data["С/%"].str.replace("−","-").str.replace("%", "").astype(float)

# выставляем индекс по сделкам по процентам и отсортируем:

data = data.set_index("С/%")
data = data.sort_index(ascending = False) # сортировка по возрастаниюб ascending = True - сортировка по 
# возрастанию, начиная с отрицательных значений,  а ascending = False - начиная с положительных

print(data["Тикер"].head(1)) # нужно вывести первое значение серии "Тикер" в уже отсортированных данных
```
____

## 7. Задание: парсинг интернет-магазина

Используя парсинг данных с маркетплейса beru.ru, найдите, на сколько литров отличается общий объем холодильников Саратов 263 и Саратов 452?
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу:
video.ittensive.com/data/018-python-advanced/beru.ru/

### Решение: 

```python
Используя парсинг данных с маркетплейса beru.ru, найдите, на сколько литров отличается общий объем холодильников Саратов 263 и Саратов 452?
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу: video.ittensive.com/data/018-python-advanced/beru.ru/

import requests
from bs4 import BeautifulSoup

# для начала нужно отправная точка, это поиск по слову "Саратов" на сайте беру и выбор необходимых 
# холодильников. Получаем ссылки на эти страницы и получаем данные. 

# пишем имя нашего бота
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 YaBrowser/19.12.0.358 Yowser/2.5 Safari/537.36"}
r = requests.get("https://beru.ru/search?cvredirect=2&suggest_reqId=27865074762321487883702093457804&text=%D1%81%D0%B0%D1%80%D0%B0%D1%82%D0%BE%D0%B2", headers=headers)
html = BeautifulSoup(r.content)
print (r.content) 	# смотрим на html код

# в html коде нужно найти блоки с товарами ("Холодильник Саратов 263"), и далее во всех блоках найти ссылки
# ищем ссылку на товар. 

links = html.find_all("a", {"class": "grid-snippet__react-link"})
# 1. сначала указываем тег "а" 
# 2. указываем ссылку класса "class": "grid-snippet__react-link" на товар Саратов 263. 
# 3. В этих ссылках нужно найти определенный текст "grid-snippet__react-link"

link_263 = ''
link_452 = ''
for link in links:     # перебираем ссылки и ищем определенный текст 
    if str(link).find("Саратов 263") > -1:      # приводим ссылку к строке str и найти текст "Саратов 263" 
        # и если текст найден "> -1", то соответствующая ссылка будет равно атрибута href из html(т.е. нужная ссылка заключена в атрибуте href)
        
        link_263 = link["href"]  	# делам то же самое для Саратова 452
    if str(link).find("Саратов 452") > -1:
        link_452 = link["href"]
       
def find_volume (link): 
    # Мы будем получать один и тот же контент с двух страниц. Поэтому нужно написать получение 
    # контента с одной страницы и затем обернем это в функцию, для того, чтобы не дублировать код
    
    r = requests.get("https://beru.ru" + link)
    html = BeautifulSoup(r.content)
    volume = html.find_all("span", {"class": "_112Tad-7AP"}) # находим span с классом в коде для объема холодильника "_112Tad-7AP"
    
    # далее выделяем число 195 из текста "общий объем 195 литров":
    
    return int(''.join(i for i in volume[2].get_text() if i.isdigit())) # - сделаем генератор:
# 1. i for i in volume[2] - проверим все символы в строке "общий объем 195 литров" (volume[2] - третий из найденных текстов)
# 2. получаем текст  .get_text()
# 3. if i.isdigit() и символы яв-ся цифрами, то возвращаем эти цифры 
# 4. ''.join - объединяем полученный список через пустую строку "" и переводим к целому числу

if link_263 and link_452: # получаем контент с этих ссылок. Мы будем получать один и тот же контент 
    # с двух страниц. Поэтому нужно написать получение контента с одной страницы и затем обернем это в 
    # функцию, для того, чтобы не дублировать код
    volume_263 = find_volume(link_263) # для каждой ссылки находим объем
    volume_452 = find_volume(link_452)  
    diff = max(volume_263, volume_452) - min(volume_263, volume_452)  # выводим разницу между двумя значениями  через нахождение максимального и минимального значений у двух объемов
    print (diff)
```
____

## 8. Задание: загрузка результатов в БД

Соберите данные о моделях холодильников Саратов с маркетплейса beru.ru: URL, название, цена, размеры, общий объем, объем холодильной камеры.
Создайте соответствующие таблицы в SQLite базе данных и загрузите полученные данные в таблицу beru_goods.
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу:
video.ittensive.com/data/018-python-advanced/beru.ru/

### Решение:

```python
import sqlite3
import requests
from bs4 import BeautifulSoup

Сначала нужно получить характеристики товара (длина х высота х габариты), для создания полей(таблицы). Нужно пройтись по всем товарам и выгрузить в таблицу.
Начнем с отработки url конкретного товара(парсить), потомучто мы получим ссылки в каталоге такого же вида. 
Для этого введем функцию find-data:

def find_number(text):   # функция для нахождения "общий объем"- приведение к целому числу некоторой строки, которую джойним через пустую строку "", если все символы яв-ся числами (т.е. извлекаем только цифры
    return int("0" + "".join(i for i in text if i.isdigit()))
def find_data (link):
    r = requests.get("https://beru.ru" + link)  

# - получаем контент с сайта беру.ру с указанной ссылкой, и нужно просто поставить домен с протоколом, чтобы получить 
    
    html = BeautifulSoup(r.content)
    title = html.find("h1", {"class": "_3TfWusA7bt"}).get_text() 

# находим название холодильника с классом, берем у него текст, потому что тегов h1 м.б. несколько
    # класс _3TfWusA7bt только один и поэтому лучше позиционироваться по этому классу. (Но можно было и без класса, найдя только первый тег h1)
    
    price = find_number(html.find("span", {"data-tid": "c3eaad93"}).get_text()) 

# найдем цену: найдем по тегу data-tid -c3eaad93 цену.(возьмем первый тег span c этим классом c3eaad93)
    
    tags = html.find_all("span", {"class": "_112Tad-7AP"}) # найдем все параметры ШхВхГ. Для этого спозиционируемся по span "_112Tad-7AP",  таких спанов 4, которые охватывают эти параметры
    
    # заведем переменные параметров:
    
    width = 0
    depth = 0
    height = 0
    volume = 0  	# объем холодильной камеры
    freezer = 0 	# объем морозильной камеры
    
    # Далее переберем теги,  вычленим эти параметры из тегов:
    for tag in tags:
        tag = tag.get_text()  # берем текст из тега
        if tag.find("ШхВхГ") > -1:  # если в теге есть строка "ШхВхГ", то эту строку нужно распарсить: разделяем по двоеточию(":")
                                   
            dims = tag.split(":")[1].split("х")  # и взять вторую часть из двоеточия с индексом [1], и разделить по разделителю "х"
           # получаем значение, которое потом приведем к числам: 

            width = float(dims[0])   # - первое значение из dims 
            depth = float(dims[1])   # - второе значение из dims
            height = float(dims[2].split(" ")[0])  # - третье значение из dims. Чтобы не вошли сантиметры в результат- 
            разобъем по пробелу и возьмем первый элемент [0]
        if tag.find("общий объем") > -1:  # если в теге есть "общий объем", то вычленяем из тега число объема
            volume = find_number(tag)  	 # находим через функцию find_number общий объем (tag- строка): 
            # def find_number(text): 	  # функция для нахождения "общий объем"- приведение к целому числу некоторой строки, которую джойним через пустую строку "", если все символы яв-ся числами (т.е. извлекаем только цифры     return int("0" + "".join(i for i in text if i.isdigit()))

        if tag.find("объем холодильной камеры") > -1:
            freezer = find_number(tag)  	# также находим через функцию find_number общий объем (tag- строка) 
    return [link, title, price, width, depth, height, volume, freezer] 	# вернем все то, что искали

''' Выше мы получили функцию хелпер (helper) -def find_data, которые находит нужные данные по каждому холодильнику '''

# Далее нужно взять со страницы (донора)


r=  requests.get("https://beru.ru/catalog/kholodilniki/79958/list?cvredirect=3&suggest_reqId=83526016473955609954771572320629&text=%D0%A1%D0%B0%D1%80%D0%B0%D1%82%D0%BE%D0%B2")
html = BeautifulSoup(r.content)  # нужно найти все ссылки на холодильники Саратов

links = html.find_all("a", {"class": "grid-snippet__react-link"}) # находим ссылки по тегу "а" и по классу "grid-snippet__react-link"
data = []
for link in links:
    if link["href"] and link.get_text().find("Саратов") > -1: # нужно получить атрибут href, если есть и текст ссылки содержит название "Саратов"

        data.append(find_data(link["href"]))  	# добавляем результаты работы функции "find_data" к той ссылке, которую нашли и произойдет перебор страницы беру.ру  для  получения нужных данных 
conn = sqlite3.connect("sqlite/data.db3")	# создаем подключение к БД
db = conn.cursor()

# выполняем запрос по созданию базы данных: 

db.execute("""CREATE TABLE beru_goods
            (id INTEGER PRIMARY KEY AUTOINCREMENT not null,
            url text,
            title text default '',
            price INTEGER default 0,
            width FLOAT default 0.0,
            depth FLOAT default 0.0,
            height FLOAT default 0.0,
            volume INTEGER default 0,
            freezer INTEGER default 0)""")
conn.commit()  # закоммитим результат

# После получения всех данных передаем их одним запросом на вставку данных в базу.
# Добавляем запрос в добавление в БД beru_goods и перечисляем те поля, куда добавляем url, title  и т.д.
(перечисляем поля)  и передаем в качестве значений список списков, кот. совпадают с количеством строк, 
# передаем в VALUES 8 знаков "?" и в качестве параметров передаем (data), который представляет собой список списков:

db.executemany("""INSERT INTO beru_goods (url, title, price, width, depth, height, volume, freezer)
           VALUES (?, ?, ?, ?, ?, ?, ?, ?)""", data)

# метод .executemany - если нужно добавить множество строк одновременно в БД   

conn.commit()
print (db.execute("SELECT * FROM beru_goods").fetchall()) 	# выведем все значения из созданной базы данных 
db.close() 	# закрываем БД
```
_____

## 9. Загрузите данные по ЕГЭ за последние годы

https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv 
выберите данные за 2018-2019 учебный год.
Выберите тип диаграммы для отображения результатов по административному округу Москвы, 
постройте выбранную диаграмму для количества школьников, написавших ЕГЭ на 220 баллов и выше.
Выберите тип диаграммы и постройте ее для районов Северо-Западного административного округа Москвы для
количества школьников, написавших ЕГЭ на 220 баллов и выше.

### Решение: 

```python
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
data = pd.read_csv("D:\Обучение уроки\Продвинутый PYTHON\Часть 3 ВИЗУАЛИЗАЦИЯ ДАННЫХ\Основы Matplotlib\ЕГЭ.csv", delimiter=";")

# Преобразуем данные по округу и району, чтобы устранить дубликаты и сделать подписи короче: 
# удалим из названия районов слово "район"
# а из округа удалим все слова, кроме первого. 
# Дополнительно назначим район, как категории, чтобы группировка происходила быстрее:

data["District"] = data["District"].str.replace("район ","").astype("category")
data["AdmArea"] = data["AdmArea"].apply(lambda x:x.split(" ")[0]).astype("category") 
# из названия округа берем только первое слова[0]

# выставим индекс по году и отфильтруем по индексу, чтобы получить данные только за 2018-2019 гг.:
data = data.set_index("YEAR").loc["2018-2019"].reset_index()    # сброим индекс - reset.index()
# т.к. данных по районам и округам достаточно много и вместе они образуют совокупность, 
то лучше использовать круговую диаграмму:  две круговые диаграммы в одну строку:
fig = plt.figure(figsize=(12,12))

# добавим холст:
area = fig.add_subplot(1, 2, 1)

# сначала построим распределение отличников по ЕГЭ по округам
area.set_title("ЕГЭ в Москве", fontsize=20) # заголовок
data_adm = data.set_index("AdmArea")

# отсечем данные только отличников, сгруппируем по округам и выведем сумму на круговой диаграмме:
data_adm["PASSES_OVER_220"].groupby("AdmArea").sum().plot.pie(ax=area, label="")

# для второй диаграммы выведем данные в СЗ округе, для этого добавим область для вывода данных
area = fig.add_subplot(1, 2, 2)
area.set_title("ЕГЭ в СЗАО", fontsize=20)	 # выведем завголвок "ЕГЭ в СЗАО"

#  выведем данные по СЗАО, переустановим индекс на "District"
data_district = data_adm.loc["Северо-Западный"].reset_index().set_index("District")

# отсечем только отличников и построим круговую диаграмму по районам:
data_district = data_district["PASSES_OVER_220"].groupby("District").sum() 

# выведем подписи к районам, чтобы узнать ответ. Для этого нужно вычислить общее число отличников по району. 
total = sum(data_district)  

#  Рассчитав точное значение мы могли бы привести к целому типу чисел, но тогда у нас 
потеряется точность из-за  двойного приведения значений. Поэтому обязательно, 
сначала нужно округлить значения по району и только потом привести  целому числу, 
что и дает точный ответ - 188 отличников в 2018-2019 учебном году:
data_district.plot.pie(ax=area, label="", autopct=lambda x:int(round(total * x/100)))
plt.show()
```
![Exam results](https://user-images.githubusercontent.com/96381562/169076320-1f4a4b6e-b477-4ac9-af45-e50acfa0caf3.png)
_____

## 10. Задание: результаты марафона

Загрузите данные по итогам марафона
https://video.ittensive.com/python-advanced/marathon-data.csv. 
Приведите время половины и полной дистанции к секундам.
Найдите, данные каких серии данных коррелируют (используя диаграмму pairplot в Seaborn).
Найдите коэффициент корреляции этих серий данных, используя scipy.stats.pearsonr.
Постройте график jointplot для коррелирующих данных.

### Решение: 

```python
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import scipy.stats as stats

Введем специальную функцию для конвертации времени, которая принимает на вход строку описывающую время и возвращать через  генератор, перебирающий пары значений - непосредственно число и его вес в секундах [3600, 60, 1]. Потом берем суммы этих произведений  этих чисел и их весов и вернем из функции. Значения же часов, минут и секунд получим разделив исходную строку времени по  двоеточию ":" - a.split(':')

def convert_time(a):
    return sum(x*int(t) for x,t in zip([3600, 60, 1], a.split(":")))
data = pd.read_csv("D:\\Обучение уроки\\Продвинутый PYTHON\\Часть 3 ВИЗУАЛИЗАЦИЯ ДАННЫХ\\2 Визуализация зависимостей\\marathon-data.csv", delimiter = ";")
 
age,  gender, split,        final
0  33,  M,     01:05:38,  02:08:51
1  32,  M,     01:06:26,  02:09:28
2  31,  M,     01:06:49,  02:10:42
3  38,  M,     01:06:16,  02:13:45
4  31,  M,     01:06:32,  02:13:59

# нужно построить парный график по всем числовым данным и посмотреть их взаимную корреляцию
# Еще одно сложность в том, что split  и final не представлены в виде чисел(это значения времени, в формате час, минута, секунда)
# И требуется привезти из этого формата в какое-либо число - например, к общему числу секунд. 
# И для этого ввели специальную функцию для конвертации времени 

# И теперь применим эту функцию выше к обеим сериям данных 
data["split"] = data["split"].apply(convert_time)
data["final"] = data["final"].apply(convert_time)

# построим парный график, чтобы посмотреть, чтобы посмотреть, какие данные с какими коррелируют
# Для групп(или категорий используем пол спортсменов - gender)

sns.pairplot(data, hue = "gender", height = 4)
plt.show()

# построим корреляционный график для двух переменных и найдем коеф-т корреляции Пирсона 
sns.jointplot("split", "final", data, height = 12, kind = "kde").annotate(stats.pearsonr)
plt.show()

# Дополнительно выведем коеф-т пирсона по двум сериям данным отдельно - видим изолинии для коррелирующих серий и кофе-т - 0,96 

print(round(stats.pearsonr(data["split"], data["final"])[0]), 2)
```
![marafon](https://user-images.githubusercontent.com/96381562/169078298-8c5c36ff-2a6d-4ead-91e2-f9798353ea0a.png)
____

## 11. Задание: скользящие средние на биржевых графиках

Используя данные индекса РТС за последние годы. 
https://video.ittensive.com/python-advanced/rts-index.csv
постройте отдельные графики закрытия (Close) индекса по дням за 2017, 2018, 2019 годы в единой оси X.
Добавьте на график экспоненциальное среднее за 20 дней для значения Max за 2017 год.
Найдите последнюю дату, когда экспоненциальное среднее максимального дневного значения (Max) в 2017 году 
было больше, чем соответствующее значение Close в 2019 году (это последнее пересечение графика за 2019 год и графика для среднего за 2017 год). 

### Решение:

```python
%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("D:\\Обучение уроки\\Продвинутый PYTHON\\Часть 3 ВИЗУАЛИЗАЦИЯ ДАННЫХ\\3 Временные ряды\\rts-index.csv")
С данными есть 2 сложности: 1. Нужно привести дату строку к дате(времени) и 2. Развернуть данные времени в 
обратном порядке. Потому что они организованы сначала самые свежие, а в конце самые старые. А нам нужно, чтобы у нас
вначале шли самые старые, а затем свежие в конце. 
Параметр dayfirst указывает функции парсинга, что первое в строке первым идет день, а не наоборот. 
Если не задать этот параметр, то функция может не правильно преобразовывать даты и путать месяц и день местами.  
Например, 01.02.2013 будет преобразовано в 02-01-2013, что будет неправильно. И чтобы все даты корректно отобразились 
из русского формата в английский (западный)

data['Date'] = pd.to_datetime(data['Date'], dayfirst=True) 
# Создадим дату из строки Date, дополнительно укажем параметр dayfirst:
dates = pd.date_range(min(data['Date']), max(data['Date']))

# переиндексируем все данные и заполним пустые даты предыдущими значениями. Это потребуется для сравнения
серий данных по годам,  чтобы каждый день года было хотя бы одно значение:
data = date.set.index('Date')
data = data.reindex(dates).ffill()

# Добавим еще одну серию данных - день года для подписи по осям х:
data('Day') = pd.to_datetime(data.index).dayofyear
data.index.name = 'Date'
 # назначим название индекса, которое потерялась при индексации данных. Также это м.б. сделать с помощью 
# дополнительного параметра index.

# сортируем по индексу, чтобы развернуть данные в правильном хронологическом порядке:
data = data.sort.index()

# начнем данных за 2019, создадим отдельный набор, чтобы в последствии искать искомую дату превышения, как пересечение его с набором  за 2017 год: 
data_2019 = data['2019'].reset_index().set_index('Day')

# аналогичным образом создадим набор данных для 2017 года. Для него сразу возьмем экспоненциальное 
среднее со сдвигом 20 от значения max
# И естественно, что значение экспоненциального среднего будут отличаться от исходного массива данных 
без заполненных пропусков по дням.  Но в нашей текущей задаче это несущественно, потому что в любом 
случае найдем реальное превышение значения над другим реальным значением.
# У нас будет правильный ответ по дням. Это можно сделать, не заполняя пропусками данные, а посчитать другими методами: 
data_2017 = data['2017'].reset_index().set_index('Day')['Max'].ewm(span=20).mean()

fig = plt.figure(figsize=(12,8))
area = fig.add_subplot(1,1,1)
data_2019['Close'].plot(ax=area, color='red', lebel='2019', lw=3)
data_2017.plot(ax=area, color='orange', label = 'Exp.2017', lw=3)

# нанесем данные за 2017 год в виде фоновой области:
data['2017'].reset_index().set_index('Day')['Close'].plot.area(ax=area, color='.5', label='2017', lw=3)

# данные за 2018 год в виде еще одной линии:
data["2018"].reset_index().set_index("Day")["Close"].plot(ax=area, color="blue", label="2018", lw=3)
plt.legend()
plt.show()

# На графике примерно видна уже искомая дата(это февраль-март 2019 года), когда значения индекса 
РТС окончательно превзошли максимум за 2017 год.
# Найдем точную дату: сначала отфильтруем данные за 2019 год, когда Close(закрытие периода(дня)) 
было больше максимума аналогичного дня за 2017 год :
data_fall = data_2019[data_2019['Close'] < data_2017[0:len(data_2019)]]

# выставим индекс по дате и отсортируем по индексу и выведем первое значение(т.е. последнюю дату)
data_fall.head(1), когда дневной максимум  в 2017 году был больше закрытия в соответствующем дне 2019 года. 
Также для сортировки по нему можно использовать sort.values

data_fall.set_index('Date', inplace=True)
data_fall = data_fall.sort_index(ascending=False)
print(data_fall.head(1).index)
```
![stocks](https://user-images.githubusercontent.com/96381562/169079542-d39e3ec9-0ebe-4f48-aad8-cf36ebf864ee.png)
____

## 12. Задание: объекты культурного наследия России.

Изучите набор данных по объектам культурного наследия России (в виде gz-архива):
https://video.ittensive.com/python-advanced/data-44-structure-4.csv.gz
и постройте фоновую картограмму по количеству объектов в каждом регионе России, используя гео-данные
https://video.ittensive.com/python-advanced/russia.json
Выведите для каждого региона количество объектов в нем.

### Решение: 

```python 
%matplotlib inline
import matplotlib.pyplot as plt
import geopandas as gpd
import pandas as pd
import descartes
data = pd.read_csv("data-44-structure-4.csv.gz", usecols=["Объект", "Регион"]) # используем серии (через usecols)

Приведем к верхнему регистру. Важно, чтобы регистр в обоих наборах данных был одинаковый, чтобы потом могли по региону совместить:
data["Регион"] = data["Регоин"].str.upper()

data = data.groupby("Регион").count()  # выполним группировку по региону и посчитаем число объектов
geo = gpd.read_file("russia.json")
geo = geo.to_crs({"init": "epsg:3857"})  # приведем к меркатору

Нужно замерить названия в геонаборе на нужные, которые отличаются от исходного. Таких регионов получилось 6.  Или можно заменить в исходном наборе на те названия, которые есть в geo наборе. В любом случае нужно унифицировать разные названия  и привести оба эти названия к одному названию. Для этого выведем индекс основного набора данных, т.к. он содержит все названия регионов:
# print(data.index.values).
 
И найдем все расхождения, которые у нас выведены в наборе нулевых значений в результирующем объединенном наборе данных. После этого применим replace и заменим одно название не другое:
geo = geo.replace({"ХАНТЫ-МАНСИЙСКИЙ АВТОНОМНЫЙ ОКРУГ": "ХАНТЫ-МАНСИЙСКИЙ АВТОНОМНЫЙ ОКРУГ - ЮГРА",
    "РЕСПУБЛИКА АДЫГЕЯ": "РЕСПУБЛИКА АДЫГЕЯ (АДЫГЕЯ)",
    "ЧУВАШСКАЯ РЕСПУБЛИКА": "ЧУВАШСКАЯ РЕСПУБЛИКА - ЧУВАШИЯ",
    "РЕСПУБЛИКА МАРИЙ-ЭЛ": "РЕСПУБЛИКА МАРИЙ ЭЛ",
    "РЕСПУБЛИКА СЕВЕРНАЯ ОСЕТИЯ": "РЕСПУБЛИКА СЕВЕРНАЯ ОСЕТИЯ - АЛАНИЯ",
    "РЕСПУБЛИКА ТАТАРСТАН": "РЕСПУБЛИКА ТАТАРСТАН (ТАТАРСТАН)"
                  })
После замены всех регионов снова запустим объединение данных - print (geo[geo["Объект"].isnull()]), и видим, что расхождений нет. Все регионы получили корректное число объектов культурного наследия.

Перейдем к слиянию наборов данных: наборов данных основного(data) и geo. Слияние будем выполнять по региону.  Исходный набор данных будет geo. В него вольем данные по объектам, чтобы вывести данные на картограмме:
geo = pd.merge(left = geo, right = data,
              left_on = "NL_NAME_1", right_on = "Регион", how = "left")

Чтобы найти проблему по региону, какие названия у нас разные, выведем все строки, где поле объект, т.е. число объектов в регионе NULL. Т.е. объединение корректно по этому региону не прошло, региона не было в исходном наборе(data).  Таких регионов получилось 6 - print (geo[geo["Объект"].isnull()]) 

Обрисуем фоновую картогармму: создадим холст и область обрисовки:
fig = plt.figure(figsize=(16,9))
area = plt.subplot(1, 1, 1)

Для картограммы будем использовать объект(число объектов культ. наследия для цветовой градации с пом. cmap(colormaps), позволяет выбрать цвета для карты).




geo.plot(ax=area, legend=True, column="Объект", cmap="Reds")
area.set_xlim(2e6, 2e7)	 # обрежем Чукотку и все что западнее Калининграда, для лучшей визуализации карты, т.е.  зададим лимиты по оси х (через set.xlim)координаты указаны в десятках миллионах 1е7, поэтому нужен диапазон от 2 до 10 млн.(2e6, 2e7)

Введем аннотацию в виде числа объектов в каждом регионе, перебрав все строки в результирующем наборе данных: 
for _, region in geo.iterrows():
    area.annotate(region["Объект"],
                 xy=(region.geometry.centroid.x, # число объектов разместим в центроиде региона, отвечающий за регион
                    region.geometry.centroid.y), fontsize=8)
plt.show()
print (geo[geo["NL_NAME_1"] == "АЛТАЙСКИЙ КРАЙ"]["Объект"]) 

# выведем число объектов культ.  наследия по Алтайскому краю из объединенного набора данных
11    4480
Name: Объект, dtype: int64
```
![Culture](https://user-images.githubusercontent.com/96381562/169081042-5b7d2209-9700-45dc-b1e9-8dc1bb3f44ca.png)
____

## 13. Задание: сборка PDF документа.

Используя данные по посещаемости библиотек в районах Москвы
https://video.ittensive.com/python-advanced/data-7361-2019-11-28.utf.json
постройте круговую диаграмму суммарной посещаемости (NumOfVisitors) 20 наиболее популярных районов Москвы.
Создайте PDF отчет, используя файл
https://video.ittensive.com/python-advanced/title.pdf
как первую страницу. На второй странице выведите итоговую диаграмму, 
самый популярный район Москвы и число посетителей библиотек в нем.

### Решение: 

```python
matplotlib inline
from reportlab.pdfgen import canvas
from reportlab.lib import pagesizes
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.utils import ImageReader
from PyPDF2 import PdfFileMerger, PdfFileReader
import requests
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def extract_district (x):
    return list(map(lambda a: a["District"], x))[0]

r = requests.get("https://video.ittensive.com/python-advanced/data-7361-2019-11-28.utf.json")
Т.к. данные в формате json, поэтому их сначала загружаем и передаем в  фрейм данных.  Заполним отсутствующие значения нулями .fillna: 
data = pd.DataFrame(json.loads(r.content)).fillna(value=0)

Первая сложность в том, что названия района скрыто в поле District, которое само по себе яв-ся json, разобранный в словарь, т.е.  каждое значение в серии данных это еще один набор значений в виде словаря. Для группировки по районам нужно извлечь названия района из этой структуры для этого мы создали отдельную функцию. Эта функция извлечения района будет брать значении серии Object Adress,  находить в этом значении поле District и затем приводить это поле к списку и из этого списка возвращать первое значение. Т.е  по факту будет вычленять первый найденный District из словаря ObjectAdress:
data["District"] = data["ObjectAddress"].apply(extract_district)

После получения района для всех значений данных можно сгруппировать по нему и отсортировать по числу посетителей библиотек по убыванию: 
data_sum = data.groupby("District").sum().sort_values("NumOfVisitors", ascending=False)

Создадим результирующую круговую диаграмму из 20 самых популярных районов:

fig = plt.figure(figsize=(11,6))
area = fig.add_subplot(1, 1, 1)

data_sum[0:20]["NumOfVisitors"].plot.pie(ax = area,
                                        labels=[""]*20, 	 # для большей читабельности удерем районы из подписей на диаграмме, задав пустые labels
                                         # ровно по числу районов (20)
                                        label="Посещаемость",
                                        cmap="tab20")

перенесем все районы в легенду (это будет индекс из набора данных). Легенда выйдет справа от диаграммы, задав bbox_to_anchor=(1.5,1,0.1,0))   - сдивнем по оси х 1.5, по оси у задали 1(прижали к верхней границе), также задали 0, нулевая ширина,  чтобы легенда графика корректоно отрисовалась: 

plt.legend(data_sum[0:20].index,
          bbox_to_anchor=(1.5,1,0.1,0)) 
plt.savefig("readers.png") 	# сохраняем файл для дальнейшей вставки

Сформируем отчет:

pdfmetrics.registerFont(TTFont("Trebuchet", "D://Обучение уроки//Продвинутый PYTHON//Часть 4 ОТЧЕТЫ И АВТОМАТИЗАЦИЯ//Глава 1. Работа с PDF//Trebuchet.ttf"))
PDF = canvas.Canvas("readers.pdf", pagesize=pagesizes.A4)
PDF.setFont("Trebuchet", 48)
PDF.drawString(70, 650, "Посетители библиотек")
PDF.drawString(80, 590, "по районам Москвы")
PDF.setFont("Trebuchet", 13)
PDF.drawString(550, 820, "2") # 2 страница отчета 
PDF.drawImage(ImageReader("readers.png"), -200, 150) # вставляем круговую диаграмма
PDF.setFont("Trebuchet", 20)
PDF.drawString(100, 150, "Самый популярный район")
PDF.setFont("Trebuchet", 24)
PDF.drawString(100, 120, data_sum.index.get_values()[0]) # выбираем самый популярный район 
PDF.setFont("Trebuchet", 20)
PDF.drawString(100, 90,
               "Посетителей: " + str(int(data_sum["NumOfVisitors"].values[0]))) # выведем число посетителей по первому нашему кортежу
# Это и сеть ответ задачи
PDF.save()
files = ["D://Обучение уроки//Продвинутый PYTHON//Часть 4 ОТЧЕТЫ И АВТОМАТИЗАЦИЯ//Глава 1. Работа с PDF//title.pdf", "readers.pdf"]
merger = PdfFileMerger()
for filename in files:
    merger.append(PdfFileReader(open(filename, "rb")))
merger.write("report.pdf")
```
![pdf](https://user-images.githubusercontent.com/96381562/169082249-0b5d3c08-85fc-4bfe-aa16-5a8acc640408.png)
____

## 14. Задание: геральдические символы Москвы

Сгенерируйте PDF документ из списка флагов и гербов районов Москвы:
https://video.ittensive.com/python-advanced/data-102743-2019-11-13.utf.csv
На каждой странице документа выведите название геральдического символа (Name), его описание (Description) и его изображение (Picture).
Для показа изображений используйте адрес
https://op.mos.ru/MEDIA/showFile?id=XXX
где XXX - это значение поля Picture в наборе данных. 

### Решение: 

```python

Сгенерируем PDF документ из списка флагов и гербов районов Москвы: https://video.ittensive.com/python-advanced/data-102743-2019-11-13.utf.csv На каждой странице документа выведите название геральдического символа (Name), его описание (Description) и его изображение (Picture). Для показа изображений используйте адрес https://op.mos.ru/MEDIA/showFile?id=XXX где XXX - это значение поля Picture в наборе данных. Например: https://op.mos.ru/MEDIA/showFile?id=8466da35-6801-41a9-a71e-04b60408accb В случае возникновения проблем с загрузкой изображений с op.mos.ru можно добавить в код настройку для форсирования использования дополнительных видов шифрования в протоколе SSL/TLS. requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'

Формирование pdf отчета из набора данных. Требуется правильно сформировать html документ. 
Сначала откроем заголовок и откроем содержание. В заголовке будут Геральдические символы Москвы и meta cherset="utf-8". Добавляем страничный вывод данных. Можно использовать разного уровня заголовки и создать для них стили. Можно вывести заголовки одного уровня и создать стили для всех, кроме первого(будем использовать второй вариант)
Переберм в цикле набор данных (for i, item in data.iterrows():, и только для первого элемента(заголовка) мы не будем задавать стиль. А для всех остальный зададим, вставим разрыв страницы после первого заголовка
(html += '<h1 style="page-break-before:always">' + item['Name'] + '</h1>'). Т.о. получим каждый геральдический символ с названием на отдельной странице:
    После вывода заголовка, который будет обеспечивать разрыв страницы выведем изображение геральдического символа в увеличенном виде. Для этого используем атрибут style и свойство width и margin-left (отступ слева, поле слева). Источник изображения - src=... в нужном формате и добавим к нему значения поля(столбца) Picture из исходных данных
    ''<p>
        <img style="width:80%;margin-left:10%"
        src="https://op.mos.ru/MEDIA/showFile?id=''' + item['Picture'] + '''">
    </p>'''
Далее выведем описание символа и увеличим шрифт в 1,5 раза- '<p style="font-size:150%"> и добавим свойство Description у кортежа. HTML готов для формирования.
        Зададим размер страницы и вывод каждой страницы отдельно  'page-size': 'A4', 'header-right': '[page]' 
	и сгенерируем из строки через .from_string сохраним, имя файла.


import pandas as pd
import pdfkit
data = pd.read_csv("https://video.ittensive.com/python-advanced/data-102743-2019-11-13.utf.csv", delimiter=";")
html = '''<html>
<head>
    <title>Геральдические символы Москвы</title>
    <meta charset="utf-8"/>
</head>
<body>'''
for i, item in data.iterrows():
    if i == 0:
        html += '<h1>' + item['Name'] + '</h1>'
    else:
        html += '<h1 style="page-break-before:always">' + item['Name'] + '</h1>'
    html += '''<p>
        <img style="width:80%;margin-left:10%"
        src="https://op.mos.ru/MEDIA/showFile?id=''' + item['Picture'] + '''">
    </p>'''
    html += '<p style="font-size:150%">' + item['Description'] + '</p>'
html += '</body></html>'

config = pdfkit.configuration(wkhtmltopdf='C:/Program Files/wkhtmltopdf/bin/wkhtmltopdf.exe')
options = {
    'page-size': 'A4',
    'header-right': '[page]'
}
pdfkit.from_string(html, 'heraldic.pdf',
                    configuration=config, options=options)
		    
Loading pages (1/6)
Counting pages (2/6)                                               
Resolving links (4/6)                                                       
Loading headers and footers (5/6)                                           
Printing pages (6/6)
Done  
```
____

## 15. Задание: многостраничный отчет.

Используя данные по активностям в парках Москвы
https://video.ittensive.com/python-advanced/data-107235-2019-12-02.utf.json
Создайте PDF отчет, в котором выведите:
1. Диаграмму распределения числа активностей по паркам, топ10 самых активных
2. Таблицу активностей по всем паркам в виде Активность-Расписание-Парк

###  Решение: 

```python 
import requests
import json 
import pandas as pd
import matplotlib.pyplot as plt
from io import BytesIO
import binascii
import pdfkit

Загружаем данные в датафрейм,  возьмем 3 только колонки для вывода отчета  извлекаем названия парка из серии через лямбду функцию (получаем значения поля value у словаря) data["NameofPark"].apply(lambda x: x["value"]):

r = requests.get("https://video.ittensive.com/python-advanced/data-107235-2019-12-02.utf.json")
data = pd.DataFrame(json.loads(r.content),
                   columns=["CourseName", "CoursesTimetable", "NameOfPark"])
data["NameOfPark"] = data["NameOfPark"].apply(lambda x: x["value"])

Переименуем колонки для отчета.
data.columns = ["Активность", "Расписание", "Парк"]

Для ответа на поставленный вопрос найдем активность Тайцзицюань и выведем число записей с этой активностью: активность равна 1
print("Тайцзицюань: ",
     data[data["Активность"].str.contains("Тайцзицюань")]["Активность"].count())
fig = plt.figure(figsize=(12, 6))
area = fig.add_subplot(1,1,1)

Сгруппируем по названию парков по убыванию, чтобы всзять первые 20 значений (самые активные парки и нанести их в круговую диаграмму)
parks = data.groupby("Парк").count().sort_values("Активность", ascending=False)

Первый ньюанс - вывод изображения в отчет. Можно выбрать стандартный способ - сохранить изображения в файл, 
загрузить его из файла ( временный файл потом удалить). Но будет использован BytesIO для временного хранения 
бинарных данных изображения.  И его дальнейшего преобразования в base64 формат:
parks.head(10)["Активность"].plot.pie(ax=area, label="")
plt.show()

После сохранения изображения по этому указателю можно его преобразовать в бинарные данные в base64 формату декодированные в utf-8:
img = BytesIO()
plt.savefig(img)
img = "data:image/png;base64," + binascii.b2a_base64(img.getvalue(),
                                                    newline=False).decode("UTF-8")
Второй ньюанс - pandas ограничивает по умолчанию длину в ячейках, поэтому зададим настройку через set_option для вывода полного расписания активности каждого парка в html таблице:

pd.set_option("display.max_colwidth", 1000)
html = '''<ntml>
<head>
        <title>Активности в парках Москвы</title>
        <meta charset="utf-8/">
    </head>
<body>
<h1>Активности в парках Москвы</h1>
<img src="''' + img + '''" alt = "Популярные парки/">
        '''+ data.to_html(index=False) + '''
        </body>
</html>'''
config  = pdfkit.configuration(wkhtmltopdf = "C:/Program Files/wkhtmltopdf/bin/wkhtmltopdf.exe")
options = {
    'page-size': 'A4',
    'header-right': '[page]'
}
pdfkit.from_string(html, 'parks.pdf',
                  configuration = config, options=options)
with open("parks.html", "w", encoding = "utf-8") as file:
    file.write(html)
    
Тайцзицюань:  1
Loading pages (1/6)
Counting pages (2/6)                                               
Resolving links (4/6)                                                       
Loading headers and footers (5/6)                                           
Printing pages (6/6)
Done   
```
![final](https://user-images.githubusercontent.com/96381562/169085315-82a92bed-edd0-4a86-b645-28985fb582b4.png)
____

## 16. Задание: автоматические отчеты.

Соберите отчет по результатам ЕГЭ в 2018-2019 году, используя данные
https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv
и отправьте его в HTML формате по адресу support@ittensive.com, используя только Python.
В отчете должно быть:
* общее число отличников (учеников, получивших более 220 баллов по ЕГЭ в Москве),
* распределение отличников по округам Москвы,
* название школы с лучшими результатами по ЕГЭ в Москве.
* Диаграмма распределения должна быть вставлена в HTML через data:URI формат (в base64-кодировке).

### Решение: 

```python
import pandas as pd
import matplotlib.pyplot as plt
import pdfkit
from io import BytesIO
import binascii
import smtplib
from email import encoders
from email.mime.text import MIMEText 
from email.mime.base import MIMEBase 
from email.mime.multipart import MIMEMultipart

data = pd.read_csv("https://video.ittensive.com/python-advanced/data-9722-2019-10-14.utf.csv", delimiter = ";")

# по условию задачи выделим 2018-2019 года по условию задачи. Сначала сделаем все преобразования с данными и только потом вставим  результаты в отчет:
data = data[data["YEAR"] == "2018-2019"]

# Сначала нужно найти лучшую школу по результатам ЕГЭ - возьмем первое значение из сортированных значений по уменьшению (ascending = False).head(1) по колонке PASSES_OVER_220:
data_best = data.sort_values("PASSES_OVER_220", ascending = False).head(1)

# Сгруппируем по административному округу(предварительно уберем из названий административного округа все слова, кроме первого)  для того, чтобы подписи данных были короче и красивее:

data["AdmArea"] = data["AdmArea"].apply(lambda x: x.split(" ")[0])
# Сортировку значений по адм округам мы выполнили только для того, чтобы 2 самых маленьких значений (2 самых малочисленных округа)  на графике вынести из графика, чтобы подписи поместились. 

# Посчитаем общее количество отличников, нужно для отчета и вывода доли
data_adm = data.groupby("AdmArea").sum()["PASSES_OVER_220"].sort_values()
total = data_adm.sum()

# Далее приступим к визуализации, создадим холст и зададим список explode - список секторов и меру выноса из основной диаграммы, чтобы показать, что эти округа самые малочисленные:

fig = plt.figure(figsize=(11,6))
area = fig.add_subplot(1,1,1)
explode = [0]*len(data_adm)
explode[0] = 0.4
explode[1] = 0.4


# создадим круговую диаграмму по округам по числу округов
data_adm.plot.pie(ax=area, 
                labels=[""]*len(data_adm),
                 label="Отличники по ЕГЭ",
                 cmap="tab20",
                 autopct=lambda x:int(round(total * x/100)),
                 pctdistance=0.9,
                 explode=explode)
plt.legend(data_adm.index, bbox_to_anchor=(1.5, 1, 0.1, 0)) # легенда с подписями округов
img = BytesIO() # создаем объект памяти для изображения и сохраним в изображение
plt.savefig(img)

# для вставки изображения в отчет преобразем его в формат base64:
img = 'data:image/png;base64,' + binascii.b2a_base64(img.getvalue(),
                                newline=False).decode("UTF-8")

# для корректного вывода длинного названия школы зададим настройку pandas по длине колонки  
pd.set_option("display.max_colwidth", 1000)

# Сформируем html отчет со всем данными:

html ='''<html>
<head>
    <title>Результаты по ЕГЭ по Млскве: отличники</title>
    <meta charset= "utf-8/">
</head>
<body>
    <h1>Результаты ЕГЭ Москвы: отличники в 2018-2019 году</h1>
    <p>Всего: ''' + str(total) + '''</p>
    <img src=" ''' + img + '''" alt="Отличники по округам"/>
    <p>Лучшая школа: ''' + str(data_best["EDU_NAME"].values[0]) + '''</p>
</body>
<html>'''

# сформируем pdf отчет из html кода

config = pdfkit.configuration(wkhtmltopdf="C:/Program Files/wkhtmltopdf/bin/wkhtmltopdf.exe")
options = {
    "page-size": "A4",
    "header-right": "[page]"
}

# вызовем генерацию  pdf документа из строки и сохраним его в файл ege.best.pdf:
pdfkit.from_string(html, 'ege.best.pdf',
                  configuration=config, options=options)


# начнем подготовку отправки письма с отчетами. Создадим новый объект и зададим поля 
# From, Subject, Content-Type, To

letter = MIMEMultipart()
letter["From"] = "anatoliy.masterr@gmail.com"
letter["Subject"] = "Результаты по ЕГЭ в Москве"
letter["Content-Type"] = "text/html; charset=utf-8"
letter["To"] = "anatoli.evdokimow@yandex.ru"

# прикрепляем html документ в тело письма и вложим pdf отчет:
letter.attach(MIMEText(html, "html"))
attachement = MIMEBase("application", "pdf")
attachement.set_payload(open("ege.best.pdf", "rb").read())
attachement.add_header("Content-Disposition",
                      'attachement; filename="ege.best.pdf"')
encoders.encode_base64(attachement)
letter.attach(attachement)

# далее подключимся к почтовому серверу и отправим письмо:

user = 'anatoliy.masterr@gmail.com'
password = "teaching_python2022"
server = smtplib.SMTP("smtp.gmail.com", 587)
server.starttls()
server.login(user, password)
server.sendmail("anatoliy.masterr@gmail.com",
               "anatoli.evdokimow@yandex.ru",
               letter.as_string())
server.quit()
```
![Email](https://user-images.githubusercontent.com/96381562/169086083-f18ea1c3-b1d5-4d36-b8ae-ff0c8d4611e6.png)


